from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments, Trainer, default_data_collator
from peft import get_peft_model, LoraConfig, TaskType
from datasets import load_dataset
import torch
import os
import pandas as pd
import matplotlib.pyplot as plt

# === æ¨¡å‹åç§°å’Œè·¯å¾„ ===
base_model = "meta-llama/Meta-Llama-3.1-8B-Instruct"
data_path = "/srv/scratch/z5485311/GDL2NL/LLM/llama_finetune/llama_gdl_finetune_dataset.json"  # â† ä½ ç”Ÿæˆçš„æ•°æ®é›†è·¯å¾„
output_dir = "/srv/scratch/z5485311/GDL2NL/LLM/llama_finetune/llama3_finetuned_2"

# === åŠ è½½ tokenizer å’Œæ¨¡å‹ ===
tokenizer = AutoTokenizer.from_pretrained(base_model)
tokenizer.pad_token = tokenizer.eos_token  # é¿å… padding æŠ¥é”™

dataset = load_dataset("json", data_files=data_path)["train"]

def format(batch):
    input_ids_list = []
    labels_list = []

    for ins, inp, out in zip(batch["instruction"], batch["input"], batch["output"]):
        prompt = f"### Instruction:\n{ins}\n\n### Input:\n{inp}\n\n### Response:\n"
        prompt_tokens = tokenizer(prompt, truncation=True, max_length=2048, add_special_tokens=False)
        output_tokens = tokenizer(out, truncation=True, max_length=2048, add_special_tokens=False)

        input_ids = prompt_tokens["input_ids"] + output_tokens["input_ids"]
        input_ids = input_ids[:4096]
        labels = [-100] * len(prompt_tokens["input_ids"]) + output_tokens["input_ids"]
        labels = labels[:4096]

        # padding
        pad_len = 4096 - len(input_ids)
        input_ids += [tokenizer.pad_token_id] * pad_len
        labels += [-100] * pad_len

        input_ids_list.append(input_ids)
        labels_list.append(labels)

    return {"input_ids": input_ids_list, "labels": labels_list}

# === æ‰§è¡Œ mapï¼ˆbatch_size=32ï¼‰===
tokenized_dataset = dataset.map(
    format,
    batched=True,
    batch_size=32,
    remove_columns=dataset.column_names
)

model = AutoModelForCausalLM.from_pretrained(
    base_model,
    device_map="auto",
    torch_dtype=torch.bfloat16
)

lora_config = LoraConfig(
    r=8,
    lora_alpha=16,
    lora_dropout=0.05,
    bias="none",
    task_type=TaskType.CAUSAL_LM
)

model = get_peft_model(model, lora_config)
model.print_trainable_parameters()  # ğŸ” æ˜¾ç¤ºå“ªäº›å‚æ•°æ˜¯å¯è®­ç»ƒçš„ï¼ˆLoRA adapterï¼‰

training_args = TrainingArguments(
    output_dir=output_dir,
    num_train_epochs=5,
    per_device_train_batch_size=1,
    gradient_accumulation_steps=4,
    learning_rate=5e-6,
    logging_dir=output_dir + "/logs",
    logging_steps=10,
    save_strategy="epoch",
    bf16=True,                    # âœ… æ¨èä½¿ç”¨ bf16
    fp16=False,                   # âŒ ç¦ç”¨ fp16 é¿å… unscale æŠ¥é”™
    gradient_checkpointing=False,  # âœ… H200æ˜¾å­˜è¶³å¤Ÿå¯ä»¥å…³æ‰
    max_grad_norm=1.0,            # æˆ–è®¾ç½®ä¸º 0.0ï¼ˆå…³é—­ clipï¼‰
    label_names=["labels"],
    report_to="none"
)


trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_dataset,
    tokenizer=tokenizer,
    data_collator=default_data_collator,
)

trainer.train()

model.save_pretrained(output_dir)

loss_logs = [x for x in trainer.state.log_history if 'loss' in x]
df = pd.DataFrame(loss_logs)  # é€šå¸¸åŒ…å«: step, loss, learning_rate, epoch

# å¯é€‰ï¼šä¿å­˜åŸå§‹æ—¥å¿—ä¸º CSVï¼Œä¾¿äºå¤ç”¨
df.to_csv(f"{output_dir}/loss_history.csv", index=False)

# ç»˜å›¾ï¼ˆæ³¨æ„ï¼šä¸è¦æŒ‡å®šé¢œè‰²ï¼›ä¸€å¼ å›¾ä¸€ä¸ªæ›²çº¿ï¼‰
plt.figure()
plt.plot(df["step"], df["loss"])
plt.xlabel("Step")
plt.ylabel("Training Loss")
plt.title("Supervised Fine-Tuning Loss (LLaMA 3.1-8B-Instruct)")
plt.grid(True, alpha=0.3)
plt.tight_layout()
plt.savefig(f"{output_dir}/loss_curve.png", dpi=200)